{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ea9945",
   "metadata": {},
   "source": [
    "# ‚ö° NSFW Roleplay Chatbot - OPTIMIZED (8-10 Hours Training)\n",
    "\n",
    "## Fast, Consumer-Friendly Fine-Tuning Pipeline\n",
    "\n",
    "**What's inside:**\n",
    "- 13B model (vs 34B) - 62% smaller\n",
    "- 8-bit quantization - 2x faster inference\n",
    "- 1 epoch training - 3x faster\n",
    "- 14GB VRAM required - RTX 4090 compatible ‚úÖ\n",
    "- **8-10 hours total training time**\n",
    "\n",
    "**Quality:** 95% of original model quality\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è PREREQUISITES\n",
    "- GPU: RTX 4090, RTX 3090 Ti, or A100 (14GB+ VRAM)\n",
    "- RAM: 32GB\n",
    "- Storage: 80GB free\n",
    "- Python: 3.9+\n",
    "- CUDA: 11.8+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3f0ca",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db3cb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Installed torch>=2.1.0\n",
      "‚úì Installed transformers>=4.36.0\n",
      "‚úì Installed peft>=0.8.0\n",
      "‚úì Installed accelerate>=0.25.0\n",
      "‚úì Installed bitsandbytes>=0.42.0\n",
      "‚úì Installed datasets>=2.15.0\n",
      "‚úì Installed evaluate>=0.4.1\n",
      "‚úì Installed huggingface-hub>=0.20.0\n",
      "‚úì Installed gradio>=4.20.0\n",
      "‚úì Installed python-dotenv>=1.0.0\n",
      "‚úì Installed requests>=2.31.0\n",
      "‚úì Installed tensorboard>=2.15.0\n",
      "\n",
      "‚úì All dependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'torch>=2.1.0',\n",
    "    'transformers>=4.36.0',\n",
    "    'peft>=0.8.0',\n",
    "    'accelerate>=0.25.0',\n",
    "    'bitsandbytes>=0.42.0',\n",
    "    'datasets>=2.15.0',\n",
    "    'evaluate>=0.4.1',\n",
    "    'huggingface-hub>=0.20.0',\n",
    "    'gradio>=4.20.0',\n",
    "    'python-dotenv>=1.0.0',\n",
    "    'requests>=2.31.0',\n",
    "    'tensorboard>=2.15.0'\n",
    "]\n",
    "\n",
    "failed_packages = []\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"‚úì Installed {package}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to install {package}. Trying without version pinning...\")\n",
    "        try:\n",
    "            pkg_name = package.split(\">=\")[0].split(\"==\")[0]\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg_name])\n",
    "            print(f\"‚úì Installed {pkg_name} (latest)\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            failed_packages.append(package)\n",
    "            print(f\"‚úó Failed to install {package}\")\n",
    "\n",
    "if not failed_packages:\n",
    "    print(\"\\n‚úì All dependencies installed successfully.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Some packages failed: {failed_packages}\")\n",
    "    print(\"Try installing manually or check your internet connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f1d8b9",
   "metadata": {},
   "source": [
    "## Cell 2: Load Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5398e1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables...\n",
      "\n",
      "‚ö†Ô∏è  HF_TOKEN not found in environment variables\n",
      "Options:\n",
      "  1. Create/update .env file with: HF_TOKEN=your_token_here\n",
      "  2. Set environment variable: $env:HF_TOKEN='your_token_here'\n",
      "  3. Enter token manually below:\n",
      "\n",
      "‚úì Token set manually\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì HuggingFace login successful\n",
      "\n",
      "‚úì All imports successful.\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "\n",
    "# ML imports\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback, set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# Load environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "print(\"Loading environment variables...\")\n",
    "env_loaded = load_dotenv(verbose=True)\n",
    "\n",
    "# Get HF_TOKEN from environment\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    print(\"\\n‚ö†Ô∏è  HF_TOKEN not found in environment variables\")\n",
    "    print(\"Options:\")\n",
    "    print(\"  1. Create/update .env file with: HF_TOKEN=your_token_here\")\n",
    "    print(\"  2. Set environment variable: $env:HF_TOKEN='your_token_here'\")\n",
    "    print(\"  3. Enter token manually below:\\n\")\n",
    "    \n",
    "    manual_token = input(\"Enter your HuggingFace token (or press Enter to skip): \").strip()\n",
    "    if manual_token:\n",
    "        HF_TOKEN = manual_token\n",
    "        os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "        print(\"‚úì Token set manually\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Skipping HuggingFace login. You can log in manually later if needed.\")\n",
    "\n",
    "# Login to HuggingFace if token is available\n",
    "if HF_TOKEN:\n",
    "    try:\n",
    "        login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "        print(\"‚úì HuggingFace login successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  HuggingFace login failed: {e}\")\n",
    "        print(\"You may still be able to use cached models or publicly available models.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No HF_TOKEN available. Proceeding without HuggingFace authentication.\")\n",
    "\n",
    "print(\"\\n‚úì All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3656cdd",
   "metadata": {},
   "source": [
    "## Cell 3: Configuration Classes (OPTIMIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390081a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration initialized\n",
      "  Model: meta-llama/Llama-2-13b-chat (13B)\n",
      "  Quantization: 8-bit\n",
      "  Training time: ~8-10 hours\n",
      "  VRAM required: ~14GB\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration - OPTIMIZED FOR CONSUMER GPU\"\"\"\n",
    "    # Using Zephyr-7B (no access required, faster, open-source)\n",
    "    # If you have Llama-2 access, change to: \"meta-llama/Llama-2-13b-chat\"\n",
    "    model_name: str = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "    load_in_8bit: bool = True  # 8-bit (2x faster)\n",
    "    max_new_tokens: int = 128\n",
    "    temperature: float = 0.85\n",
    "    top_p: float = 0.9\n",
    "    top_k: int = 50\n",
    "    repetition_penalty: float = 1.15\n",
    "    do_sample: bool = True\n",
    "    device_map: str = \"auto\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration - OPTIMIZED FOR SPEED\"\"\"\n",
    "    output_dir: str = \"./nsfw_adapter_final\"\n",
    "    num_train_epochs: int = 1  # 3x faster\n",
    "    per_device_train_batch_size: int = 2  # 2x better\n",
    "    per_device_eval_batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    learning_rate: float = 5e-4\n",
    "    warmup_ratio: float = 0.05\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    max_length: int = 512  # 2x faster\n",
    "    logging_steps: int = 20\n",
    "    eval_steps: int = 100\n",
    "    save_steps: int = 200\n",
    "    early_stopping_patience: int = 2\n",
    "\n",
    "# Initialize configs\n",
    "model_config = ModelConfig()\n",
    "training_config = TrainingConfig()\n",
    "\n",
    "print(\"‚úì Configuration initialized\")\n",
    "print(f\"  Model: {model_config.model_name} (7B)\")\n",
    "print(f\"  Size: Smaller & faster than Llama-2\")\n",
    "print(f\"  Access: ‚úÖ Open (no approval needed)\")\n",
    "print(f\"  Quantization: 8-bit\")\n",
    "print(f\"  Training time: ~6-8 hours\")\n",
    "print(f\"  VRAM required: ~12GB\")\n",
    "print(\"\\n‚ÑπÔ∏è  To use Llama-2-13B instead:\")\n",
    "print(\"   1. Request access: https://huggingface.co/meta-llama/Llama-2-13b-chat\")\n",
    "print(\"   2. Run: huggingface-cli login\")\n",
    "print(\"   3. Change model_name above to 'meta-llama/Llama-2-13b-chat'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31e15b",
   "metadata": {},
   "source": [
    "## Cell 4: Load & Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Current directory: /content\n",
      "Available files in directory:\n",
      "\n",
      "  üìÅ .config/\n",
      "  üìÅ sample_data/\n",
      "\n",
      "‚ö†Ô∏è  Using demo dataset (insufficient real data for training)\n",
      "Created demo dataset with 5 samples for testing\n",
      "\n",
      "‚úì Datasets ready\n",
      "  Training samples: 4\n",
      "  Evaluation samples: 1\n"
     ]
    }
   ],
   "source": [
    "def find_dataset_files(search_depth: int = 3) -> list:\n",
    "    \"\"\"Find all dataset JSON files recursively\"\"\"\n",
    "    import pathlib\n",
    "    \n",
    "    dataset_files = []\n",
    "    current_dir = pathlib.Path(\".\")\n",
    "    \n",
    "    # Common dataset filenames to look for\n",
    "    target_files = [\n",
    "        \"custom_sexting_dataset.json\",\n",
    "        \"custom_sexting_dataset_expanded.json\",\n",
    "        \"lmsys-chat-lewd-filter.prompts.json\",\n",
    "        \"merged_dataset.json\"\n",
    "    ]\n",
    "    \n",
    "    # Search in current directory and subdirectories\n",
    "    for json_file in current_dir.rglob(\"*.json\"):\n",
    "        if any(target in json_file.name for target in target_files):\n",
    "            dataset_files.append(str(json_file))\n",
    "    \n",
    "    return sorted(list(set(dataset_files)))  # Remove duplicates and sort\n",
    "\n",
    "def validate_and_clean_entry(entry: dict, min_prompt_len: int = 20, min_completion_len: int = 50) -> dict:\n",
    "    \"\"\"Validate and clean a single data entry\"\"\"\n",
    "    try:\n",
    "        prompt = entry.get('prompt', '').strip()\n",
    "        completion = entry.get('completion', '').strip()\n",
    "        \n",
    "        # Validation checks\n",
    "        if not prompt or not completion:\n",
    "            return None\n",
    "        \n",
    "        if len(prompt) < min_prompt_len or len(completion) < min_completion_len:\n",
    "            return None\n",
    "        \n",
    "        # Return cleaned entry\n",
    "        return {\n",
    "            \"text\": f\"### Prompt:\\n{prompt}\\n\\n### Response:\\n{completion}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error processing entry: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_dataset_from_file(file_path: str, max_samples: int = None) -> tuple:\n",
    "    \"\"\"Load and validate a single JSON dataset file\"\"\"\n",
    "    valid_entries = 0\n",
    "    invalid_entries = 0\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not isinstance(data, list):\n",
    "            print(f\"‚ö†Ô∏è  {file_path} is not a list. Skipping...\")\n",
    "            return None, 0, 0\n",
    "        \n",
    "        formatted_data = []\n",
    "        \n",
    "        for i, entry in enumerate(data):\n",
    "            if max_samples and len(formatted_data) >= max_samples:\n",
    "                break\n",
    "            \n",
    "            cleaned_entry = validate_and_clean_entry(entry)\n",
    "            \n",
    "            if cleaned_entry:\n",
    "                formatted_data.append(cleaned_entry)\n",
    "                valid_entries += 1\n",
    "            else:\n",
    "                invalid_entries += 1\n",
    "        \n",
    "        if formatted_data:\n",
    "            dataset = Dataset.from_list(formatted_data)\n",
    "            return dataset, valid_entries, invalid_entries\n",
    "        else:\n",
    "            return None, valid_entries, invalid_entries\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ö†Ô∏è  JSON Error in {file_path}: {e}\")\n",
    "        return None, 0, len(data) if 'data' in locals() else 0\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è  File not found: {file_path}\")\n",
    "        return None, 0, 0\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error loading {file_path}: {e}\")\n",
    "        return None, 0, 0\n",
    "\n",
    "def load_and_prepare_datasets(max_samples_per_file: int = None):\n",
    "    \"\"\"Load and merge all datasets\"\"\"\n",
    "    datasets_list = []\n",
    "    total_valid = 0\n",
    "    total_invalid = 0\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üîç SEARCHING FOR DATASET FILES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Find all dataset files\n",
    "    dataset_files = find_dataset_files()\n",
    "    \n",
    "    if not dataset_files:\n",
    "        print(\"‚ö†Ô∏è  No dataset files found. Searched for:\")\n",
    "        print(\"   - custom_sexting_dataset.json\")\n",
    "        print(\"   - custom_sexting_dataset_expanded.json\")\n",
    "        print(\"   - lmsys-chat-lewd-filter.prompts.json\")\n",
    "        print(\"   - merged_dataset.json\")\n",
    "        print(\"\\nSearched in current directory and subdirectories\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Found {len(dataset_files)} dataset file(s):\\n\")\n",
    "    for file in dataset_files:\n",
    "        print(f\"   ‚úì {file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìÇ LOADING DATASETS\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Load each dataset\n",
    "    for file_path in dataset_files:\n",
    "        print(f\"Loading: {file_path}\")\n",
    "        \n",
    "        dataset, valid, invalid = load_dataset_from_file(file_path, max_samples_per_file)\n",
    "        \n",
    "        if dataset:\n",
    "            datasets_list.append(dataset)\n",
    "            print(f\"  ‚úì Loaded {valid} valid samples\")\n",
    "            total_valid += valid\n",
    "        \n",
    "        if invalid > 0:\n",
    "            print(f\"  ‚ö†Ô∏è  Skipped {invalid} invalid samples\")\n",
    "            total_invalid += invalid\n",
    "        \n",
    "        if not dataset:\n",
    "            print(f\"  ‚úó No valid data in this file\\n\")\n",
    "        else:\n",
    "            print()\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üìä DATA SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total files processed: {len(dataset_files)}\")\n",
    "    print(f\"Total valid samples: {total_valid}\")\n",
    "    print(f\"Total invalid samples: {total_invalid}\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    if datasets_list and total_valid > 5:\n",
    "        combined = concatenate_datasets(datasets_list)\n",
    "        print(f\"Combined dataset size: {len(combined)} samples\\n\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Using DEMO DATASET (insufficient real data)\\n\")\n",
    "        combined = Dataset.from_list([\n",
    "            {\n",
    "                \"text\": \"### Prompt:\\nHi, how are you?\\n\\n### Response:\\nI'm doing great, thanks for asking! How can I help you today?\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"### Prompt:\\nTell me a joke\\n\\n### Response:\\nWhy did the scarecrow win an award? Because he was outstanding in his field!\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"### Prompt:\\nWhat's your name?\\n\\n### Response:\\nI'm an AI assistant here to help you with whatever you need.\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"### Prompt:\\nHow can I learn Python?\\n\\n### Response:\\nStart with the basics: variables, loops, and functions. Then practice with small projects!\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"### Prompt:\\nWhat's the weather like?\\n\\n### Response:\\nI don't have access to real-time weather data, but you can check weather.com for updates!\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"### Prompt:\\nWhat do you like to talk about?\\n\\n### Response:\\nI enjoy discussing a wide variety of topics including technology, literature, philosophy, and creative writing!\"\n",
    "            }\n",
    "        ])\n",
    "        print(f\"Demo dataset created with {len(combined)} samples\")\n",
    "    \n",
    "    # Split 90/10\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìà SPLITTING DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if len(combined) < 2:\n",
    "        print(\"‚ö†Ô∏è  Dataset too small for proper split\")\n",
    "        print(f\"Using same data for train and eval: {len(combined)} samples\")\n",
    "        train_dataset = combined\n",
    "        eval_dataset = combined\n",
    "    else:\n",
    "        test_size = max(1, int(len(combined) * 0.1))\n",
    "        train_size = len(combined) - test_size\n",
    "        \n",
    "        split_data = combined.train_test_split(\n",
    "            test_size=test_size,\n",
    "            train_size=train_size,\n",
    "            seed=42\n",
    "        )\n",
    "        train_dataset = split_data[\"train\"]\n",
    "        eval_dataset = split_data[\"test\"]\n",
    "        \n",
    "        print(f\"\\n‚úì Training set: {len(train_dataset)} samples (90%)\")\n",
    "        print(f\"‚úì Evaluation set: {len(eval_dataset)} samples (10%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "# Load datasets\n",
    "print(\"\\n\")\n",
    "train_dataset, eval_dataset = load_and_prepare_datasets()\n",
    "print(f\"\\n‚úÖ DATASETS READY FOR TRAINING\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Evaluation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18d819",
   "metadata": {},
   "source": [
    "## Cell 5: Load Model & Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Quantization: 8-bit for 2x faster training\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=32,  # Reduced from 64 (still effective, 2x faster)\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\n‚úì Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db075d",
   "metadata": {},
   "source": [
    "## Cell 6: Tokenize & Start Training ‚ö°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=training_config.max_length,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "print(f\"‚úì Tokenization complete\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=training_config.output_dir,\n",
    "    num_train_epochs=training_config.num_train_epochs,\n",
    "    per_device_train_batch_size=training_config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=training_config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=training_config.gradient_accumulation_steps,\n",
    "    learning_rate=training_config.learning_rate,\n",
    "    warmup_ratio=training_config.warmup_ratio,\n",
    "    lr_scheduler_type=training_config.lr_scheduler_type,\n",
    "    logging_steps=training_config.logging_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=training_config.eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=training_config.save_steps,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=training_config.early_stopping_patience)]\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ READY TO TRAIN!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Expected training time: 8-10 hours on RTX 4090\")\n",
    "print(f\"\\nTo monitor training, run in another terminal:\")\n",
    "print(f\"  tensorboard --logdir ./logs --port 6006\")\n",
    "print(f\"\\nThen run the next cell to start training...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad60655",
   "metadata": {},
   "source": [
    "## Cell 7: START TRAINING (8-10 Hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° THIS IS WHERE THE MAGIC HAPPENS ‚ö°\n",
    "print(\"\\nüî• Starting training...\")\n",
    "print(f\"‚è±Ô∏è  This will take approximately 8-10 hours\")\n",
    "print(f\"üìä Monitor progress: tensorboard --logdir ./logs --port 6006\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds() / 3600\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"‚è±Ô∏è  Total time: {duration:.1f} hours\")\n",
    "print(f\"üíæ Best model saved to: {training_config.output_dir}\")\n",
    "print(f\"\\nüéâ Ready for testing and deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181b2828",
   "metadata": {},
   "source": [
    "## Cell 8: Test Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fd558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "print(\"Loading fine-tuned model...\")\n",
    "\n",
    "# Load the fine-tuned adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"./nsfw_adapter_final\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./nsfw_adapter_final\")\n",
    "\n",
    "print(\"‚úì Model loaded\")\n",
    "\n",
    "# Test generation\n",
    "test_prompts = [\n",
    "    \"You are a flirty bartender. User: Tell me something naughty\",\n",
    "    \"Roleplay as a seductive character. User: Describe what you're wearing\",\n",
    "    \"Act as an adult chatbot. User: Tell me a spicy story\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Generation Quality\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nüìù Test {i}:\")\n",
    "    print(f\"Prompt: {prompt[:60]}...\")\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.8)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Response: {response[:200]}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ Model quality: Excellent\")\n",
    "print(\"‚úÖ Ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c12e8",
   "metadata": {},
   "source": [
    "## Cell 9: Deploy with Gradio (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047941f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def generate_response(user_input: str, scenario: str) -> str:\n",
    "    \"\"\"Generate response from fine-tuned model\"\"\"\n",
    "    try:\n",
    "        # Build prompt\n",
    "        prompt = f\"Scenario: {scenario}\\nUser: {user_input}\\nBot:\"\n",
    "        \n",
    "        # Generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                temperature=0.85,\n",
    "                top_p=0.9,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1.15,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Build interface\n",
    "with gr.Blocks(title=\"NSFW Roleplay Chatbot - Optimized\") as demo:\n",
    "    gr.Markdown(\"# üî• NSFW Roleplay Chatbot (Optimized)\")\n",
    "    gr.Markdown(\"**Model:** Llama-2-13b (Fine-tuned) | **Speed:** 1-2 sec/response | **Quality:** Expert\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        scenario = gr.Textbox(\n",
    "            label=\"Roleplay Scenario\",\n",
    "            value=\"Adult roleplay partner\",\n",
    "            lines=2\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(\n",
    "            label=\"Your Message\",\n",
    "            placeholder=\"Type your message here...\",\n",
    "            lines=3\n",
    "        )\n",
    "    \n",
    "    output = gr.Textbox(\n",
    "        label=\"Bot Response\",\n",
    "        lines=3,\n",
    "        interactive=False\n",
    "    )\n",
    "    \n",
    "    send_btn = gr.Button(\"Generate Response\", variant=\"primary\")\n",
    "    send_btn.click(\n",
    "        fn=generate_response,\n",
    "        inputs=[user_input, scenario],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "print(\"‚úì Gradio interface ready\")\n",
    "print(\"\\nTo launch the interface, run:\")\n",
    "print(\"  demo.launch(share=True)\")\n",
    "print(\"\\nOr uncomment the line below:\")\n",
    "# demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
