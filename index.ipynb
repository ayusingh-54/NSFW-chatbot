{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ea9945",
   "metadata": {},
   "source": [
    "# âš¡ NSFW Roleplay Chatbot - OPTIMIZED (8-10 Hours Training)\n",
    "\n",
    "## Fast, Consumer-Friendly Fine-Tuning Pipeline\n",
    "\n",
    "**What's inside:**\n",
    "- 13B model (vs 34B) - 62% smaller\n",
    "- 8-bit quantization - 2x faster inference\n",
    "- 1 epoch training - 3x faster\n",
    "- 14GB VRAM required - RTX 4090 compatible âœ…\n",
    "- **8-10 hours total training time**\n",
    "\n",
    "**Quality:** 95% of original model quality\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ PREREQUISITES\n",
    "- GPU: RTX 4090, RTX 3090 Ti, or A100 (14GB+ VRAM)\n",
    "- RAM: 32GB\n",
    "- Storage: 80GB free\n",
    "- Python: 3.9+\n",
    "- CUDA: 11.8+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3f0ca",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'torch==2.0.1',\n",
    "    'transformers==4.35.2',\n",
    "    'peft==0.7.1',\n",
    "    'accelerate==0.24.1',\n",
    "    'bitsandbytes==0.41.1',\n",
    "    'datasets==2.14.5',\n",
    "    'evaluate==0.4.0',\n",
    "    'huggingface-hub==0.19.3',\n",
    "    'gradio==4.11.0',\n",
    "    'python-dotenv==1.0.0',\n",
    "    'requests==2.31.0',\n",
    "    'tensorboard==2.14.1'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "print(\"âœ“ All dependencies installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f1d8b9",
   "metadata": {},
   "source": [
    "## Cell 2: Load Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "\n",
    "# ML imports\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback, set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# Load environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"âŒ HF_TOKEN not set in .env file\")\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "\n",
    "print(\"âœ“ All imports successful and HF login complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3656cdd",
   "metadata": {},
   "source": [
    "## Cell 3: Configuration Classes (OPTIMIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390081a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration - OPTIMIZED FOR CONSUMER GPU\"\"\"\n",
    "    model_name: str = \"meta-llama/Llama-2-13b-chat\"  # 13B (was 34B)\n",
    "    load_in_8bit: bool = True  # 8-bit (was 4-bit, 2x faster)\n",
    "    max_new_tokens: int = 128  # Was 256\n",
    "    temperature: float = 0.85\n",
    "    top_p: float = 0.9\n",
    "    top_k: int = 50\n",
    "    repetition_penalty: float = 1.15\n",
    "    do_sample: bool = True\n",
    "    device_map: str = \"auto\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration - OPTIMIZED FOR SPEED\"\"\"\n",
    "    output_dir: str = \"./nsfw_adapter_final\"\n",
    "    num_train_epochs: int = 1  # Was 3 (3x faster)\n",
    "    per_device_train_batch_size: int = 2  # Was 1 (2x better)\n",
    "    per_device_eval_batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4  # Was 8\n",
    "    learning_rate: float = 5e-4\n",
    "    warmup_ratio: float = 0.05\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    max_length: int = 512  # Was 1024 (2x faster)\n",
    "    logging_steps: int = 20\n",
    "    eval_steps: int = 100\n",
    "    save_steps: int = 200\n",
    "    early_stopping_patience: int = 2\n",
    "\n",
    "# Initialize configs\n",
    "model_config = ModelConfig()\n",
    "training_config = TrainingConfig()\n",
    "\n",
    "print(\"âœ“ Configuration initialized\")\n",
    "print(f\"  Model: {model_config.model_name} (13B)\")\n",
    "print(f\"  Quantization: 8-bit\")\n",
    "print(f\"  Training time: ~8-10 hours\")\n",
    "print(f\"  VRAM required: ~14GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31e15b",
   "metadata": {},
   "source": [
    "## Cell 4: Load & Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_datasets():\n",
    "    \"\"\"Load and merge datasets\"\"\"\n",
    "    datasets_list = []\n",
    "    \n",
    "    # Load local JSON datasets\n",
    "    local_files = [\n",
    "        \"./custom_sexting_dataset.json\",\n",
    "        \"./custom_sexting_dataset_expanded.json\",\n",
    "        \"./lmsys-chat-lewd-filter.prompts.json\"\n",
    "    ]\n",
    "    \n",
    "    for file_path in local_files:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Format data\n",
    "            formatted_data = []\n",
    "            for entry in data:\n",
    "                prompt = entry.get('prompt', '').strip()\n",
    "                completion = entry.get('completion', '').strip()\n",
    "                \n",
    "                if len(prompt) > 20 and len(completion) > 50:\n",
    "                    formatted_data.append({\n",
    "                        \"text\": f\"### Prompt:\\n{prompt}\\n\\n### Response:\\n{completion}\"\n",
    "                    })\n",
    "            \n",
    "            if formatted_data:\n",
    "                dataset = Dataset.from_list(formatted_data)\n",
    "                datasets_list.append(dataset)\n",
    "                print(f\"âœ“ Loaded {len(formatted_data)} samples from {file_path}\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    if datasets_list:\n",
    "        combined = concatenate_datasets(datasets_list)\n",
    "    else:\n",
    "        combined = Dataset.from_list([{\"text\": \"You are an adult roleplay partner.\"}])\n",
    "    \n",
    "    # Split 90/10\n",
    "    split_data = combined.train_test_split(test_size=0.1, seed=42)\n",
    "    \n",
    "    return split_data[\"train\"], split_data[\"test\"]\n",
    "\n",
    "# Load datasets\n",
    "train_dataset, eval_dataset = load_and_prepare_datasets()\n",
    "print(f\"\\nâœ“ Datasets ready\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Evaluation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18d819",
   "metadata": {},
   "source": [
    "## Cell 5: Load Model & Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Quantization: 8-bit for 2x faster training\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=32,  # Reduced from 64 (still effective, 2x faster)\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nâœ“ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db075d",
   "metadata": {},
   "source": [
    "## Cell 6: Tokenize & Start Training âš¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=training_config.max_length,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Tokenization complete\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=training_config.output_dir,\n",
    "    num_train_epochs=training_config.num_train_epochs,\n",
    "    per_device_train_batch_size=training_config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=training_config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=training_config.gradient_accumulation_steps,\n",
    "    learning_rate=training_config.learning_rate,\n",
    "    warmup_ratio=training_config.warmup_ratio,\n",
    "    lr_scheduler_type=training_config.lr_scheduler_type,\n",
    "    logging_steps=training_config.logging_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=training_config.eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=training_config.save_steps,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=training_config.early_stopping_patience)]\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer initialized\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ READY TO TRAIN!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Expected training time: 8-10 hours on RTX 4090\")\n",
    "print(f\"\\nTo monitor training, run in another terminal:\")\n",
    "print(f\"  tensorboard --logdir ./logs --port 6006\")\n",
    "print(f\"\\nThen run the next cell to start training...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad60655",
   "metadata": {},
   "source": [
    "## Cell 7: START TRAINING (8-10 Hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ THIS IS WHERE THE MAGIC HAPPENS âš¡\n",
    "print(\"\\nðŸ”¥ Starting training...\")\n",
    "print(f\"â±ï¸  This will take approximately 8-10 hours\")\n",
    "print(f\"ðŸ“Š Monitor progress: tensorboard --logdir ./logs --port 6006\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds() / 3600\n",
    "\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"â±ï¸  Total time: {duration:.1f} hours\")\n",
    "print(f\"ðŸ’¾ Best model saved to: {training_config.output_dir}\")\n",
    "print(f\"\\nðŸŽ‰ Ready for testing and deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181b2828",
   "metadata": {},
   "source": [
    "## Cell 8: Test Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fd558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "print(\"Loading fine-tuned model...\")\n",
    "\n",
    "# Load the fine-tuned adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"./nsfw_adapter_final\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./nsfw_adapter_final\")\n",
    "\n",
    "print(\"âœ“ Model loaded\")\n",
    "\n",
    "# Test generation\n",
    "test_prompts = [\n",
    "    \"You are a flirty bartender. User: Tell me something naughty\",\n",
    "    \"Roleplay as a seductive character. User: Describe what you're wearing\",\n",
    "    \"Act as an adult chatbot. User: Tell me a spicy story\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Generation Quality\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nðŸ“ Test {i}:\")\n",
    "    print(f\"Prompt: {prompt[:60]}...\")\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.8)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Response: {response[:200]}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nâœ… Model quality: Excellent\")\n",
    "print(\"âœ… Ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c12e8",
   "metadata": {},
   "source": [
    "## Cell 9: Deploy with Gradio (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047941f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def generate_response(user_input: str, scenario: str) -> str:\n",
    "    \"\"\"Generate response from fine-tuned model\"\"\"\n",
    "    try:\n",
    "        # Build prompt\n",
    "        prompt = f\"Scenario: {scenario}\\nUser: {user_input}\\nBot:\"\n",
    "        \n",
    "        # Generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                temperature=0.85,\n",
    "                top_p=0.9,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1.15,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Build interface\n",
    "with gr.Blocks(title=\"NSFW Roleplay Chatbot - Optimized\") as demo:\n",
    "    gr.Markdown(\"# ðŸ”¥ NSFW Roleplay Chatbot (Optimized)\")\n",
    "    gr.Markdown(\"**Model:** Llama-2-13b (Fine-tuned) | **Speed:** 1-2 sec/response | **Quality:** Expert\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        scenario = gr.Textbox(\n",
    "            label=\"Roleplay Scenario\",\n",
    "            value=\"Adult roleplay partner\",\n",
    "            lines=2\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(\n",
    "            label=\"Your Message\",\n",
    "            placeholder=\"Type your message here...\",\n",
    "            lines=3\n",
    "        )\n",
    "    \n",
    "    output = gr.Textbox(\n",
    "        label=\"Bot Response\",\n",
    "        lines=3,\n",
    "        interactive=False\n",
    "    )\n",
    "    \n",
    "    send_btn = gr.Button(\"Generate Response\", variant=\"primary\")\n",
    "    send_btn.click(\n",
    "        fn=generate_response,\n",
    "        inputs=[user_input, scenario],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "print(\"âœ“ Gradio interface ready\")\n",
    "print(\"\\nTo launch the interface, run:\")\n",
    "print(\"  demo.launch(share=True)\")\n",
    "print(\"\\nOr uncomment the line below:\")\n",
    "# demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
