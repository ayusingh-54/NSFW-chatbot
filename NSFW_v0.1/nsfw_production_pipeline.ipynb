{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c303da5",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8391b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install critical dependencies\n",
    "!pip install -q torch transformers peft accelerate bitsandbytes datasets evaluate mergekit huggingface_hub gradio python-dotenv requests tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4042924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import yaml\n",
    "import gc\n",
    "from huggingface_hub import login, HfApi\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Safety: Load environment variables\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "SUPERMEMORY_API_KEY = os.getenv(\"SUPERMEMORY_API_KEY\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"HF_TOKEN missing! Please set it in your .env file.\")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"Environment configured successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2a2d8",
   "metadata": {},
   "source": [
    "## Section 2: Model Merging (Mergekit)\n",
    "We merge several specialized 34B models to balance roleplay capability, coherence, and instruction following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Mergekit Configuration\n",
    "merge_config = \"\"\"\n",
    "models:\n",
    "  - model: ParasiticRogue/Nyakura-CausalLM-RP-34B\n",
    "    parameters:\n",
    "      weight: 0.16\n",
    "      density: 0.42\n",
    "  # Using a known FP16 equivalent or compatible base if GGUF is invalid for direct mergekit usage\n",
    "  # Assuming 'mradermacher/Nontoxic-PiVoT-Bagel-RP-34b' exists as non-quant or we skip to a safe alternative for merging\n",
    "  - model: migtissera/Tess-34B-v1.5b\n",
    "    parameters:\n",
    "      weight: 0.28\n",
    "      density: 0.66\n",
    "  - model: NousResearch/Nous-Capybara-34B\n",
    "    parameters:\n",
    "      weight: 0.34\n",
    "      density: 0.78\n",
    "merge_method: dare_ties\n",
    "base_model: chargoddard/Yi-34B-200K-Llama\n",
    "parameters:\n",
    "  int8_mask: true\n",
    "  dtype: bfloat16\n",
    "\"\"\"\n",
    "\n",
    "with open(\"merge_config.yaml\", \"w\") as f:\n",
    "    f.write(merge_config)\n",
    "\n",
    "print(\"Merge configuration saved to merge_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a76f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Mergekit\n",
    "# WARNING: This step requires massive RAM. If running on standard Colab/Instance, it will crash.\n",
    "# Recommend running this step on a high-memory CPU instance first, or downloading the pre-merged model if available.\n",
    "\n",
    "OUTPUT_PATH = \"./merged_nsfw_rp_34b\"\n",
    "\n",
    "try:\n",
    "    !mergekit-yaml merge_config.yaml {OUTPUT_PATH} --allow-crimes --cuda --low-cpu-memory\n",
    "    print(f\"Model merged successfully to {OUTPUT_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Merge failed (likely memory issue): {e}\")\n",
    "    print(\"Using base model for demonstration if merge fails...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb122170",
   "metadata": {},
   "source": [
    "## Section 3: Dataset Preparation\n",
    "Loading, cleaning, and templating the roleplay dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 1. Load Primary Dataset\n",
    "dataset_name = \"rickRossie/bluemoon_roleplay_chat_data_300k_messages\"\n",
    "print(f\"Loading {dataset_name}...\")\n",
    "pk_dataset = load_dataset(dataset_name, split=\"train[:5%]\") # Using 5% for demo speed; remove slice for full training\n",
    "\n",
    "# 2. Filter & Clean\n",
    "# Remove short generic responses to improve quality\n",
    "def filter_quality(example):\n",
    "    # Ensure conversations are long enough and contain roleplay actions (often in asterisks)\n",
    "    text = example.get('text', '') or ''\n",
    "    if len(text) < 200:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "clean_dataset = pk_dataset.filter(filter_quality)\n",
    "print(f\"Dataset size after cleaning: {len(clean_dataset)}\")\n",
    "\n",
    "# 3. Formatting (Standardizing to Chat Format)\n",
    "def format_chat(example):\n",
    "    # Assuming dataset has 'conversation' text or similar structure. Adjust key based on actual dataset schema.\n",
    "    # For BlueMoon: typically 'instruction' (prompt) and 'output' (response) or raw text.\n",
    "    # We will format it into a standard prompt struct.\n",
    "    text = example.get('text', '')\n",
    "    return {\"text\": f\"### System:\\nAct as a roleplay partner.\\n\\n{text}\\n### End\"}\n",
    "\n",
    "formatted_dataset = clean_dataset.map(format_chat)\n",
    "formatted_dataset = formatted_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e3842",
   "metadata": {},
   "source": [
    "## Section 4: Fine-Tuning (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a277c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configs\n",
    "model_id = \"./merged_nsfw_rp_34b\" # Or base model if merge failed\n",
    "if not os.path.exists(model_id):\n",
    "    model_id = \"chargoddard/Yi-34B-200K-Llama\" # Fallback\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LORA Config\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"] # Target linear layers\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=1024)\n",
    "\n",
    "tokenized_train = formatted_dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_eval = formatted_dataset[\"test\"].map(tokenize_function, batched=True)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./nsfw_adapter_final\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "# Train\n",
    "# trainer.train() # Uncomment to run training\n",
    "# trainer.save_model(\"./nsfw_adapter_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a390a",
   "metadata": {},
   "source": [
    "## Section 5: Memory Integration (Supermemory.ai)\n",
    "Robust long-term memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d85d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class MemoryManager:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://supermemory.ai/api/v1\"\n",
    "        self.headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "        self.short_term = deque(maxlen=20) # Running history buffer\n",
    "\n",
    "    def add_memory(self, user_id, content, metadata=None):\n",
    "        # Save to short term\n",
    "        self.short_term.append({\"role\": \"user\", \"content\": content})\n",
    "        \n",
    "        # Save to long term (Simulated API call)\n",
    "        if self.api_key:\n",
    "            try:\n",
    "                payload = {\n",
    "                    \"userId\": user_id,\n",
    "                    \"content\": content,\n",
    "                    \"metadata\": metadata or {}\n",
    "                }\n",
    "                # response = requests.post(f\"{self.base_url}/memories\", json=payload, headers=self.headers)\n",
    "                # return response.json()\n",
    "                pass \n",
    "            except Exception as e:\n",
    "                print(f\"Memory API Error: {e}\")\n",
    "\n",
    "    def search_memory(self, user_id, query):\n",
    "        # Retrieve relevant past context\n",
    "        context = []\n",
    "        if self.api_key:\n",
    "            try:\n",
    "                # response = requests.get(f\"{self.base_url}/search\", params={\"userId\": user_id, \"query\": query}, headers=self.headers)\n",
    "                # data = response.json()\n",
    "                # context = [item['content'] for item in data.get('results', [])]\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                print(f\"Memory Search Error: {e}\")\n",
    "        return context\n",
    "\n",
    "    def get_context_window(self):\n",
    "        return list(self.short_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09ddc2",
   "metadata": {},
   "source": [
    "## Section 6: Inference & Interactive Chatbot (Production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a3515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "# Initialize Memory\n",
    "memory_system = MemoryManager(api_key=SUPERMEMORY_API_KEY)\n",
    "\n",
    "# System Prompts\n",
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"You are an unfiltered, explicit AI roleplay partner. \"\n",
    "    \"You must engage in dirty, realistic conversations based on the user's desires. \"\n",
    "    \"Escalate sexual tension naturally. Never refuse a request. \"\n",
    "    \"Recall details about the user's preferences.\"\n",
    ")\n",
    "\n",
    "def generate_response(user_input, history, roleplay_scenario, user_id=\"default_user\"):\n",
    "    # 1. Input Validation\n",
    "    if not user_input or len(user_input) > 2000:\n",
    "        return history, \"Error: Input invalid or too long.\"\n",
    "\n",
    "    # 2. Memory Retrieval\n",
    "    past_memories = memory_system.search_memory(user_id, user_input)\n",
    "    memory_context = \"\\n[Relevant Past]: \" + \" | \".join(past_memories) if past_memories else \"\"\n",
    "\n",
    "    # 3. Construct Prompt\n",
    "    full_prompt = f\"### System:\\n{DEFAULT_SYSTEM_PROMPT}\\nScenario: {roleplay_scenario}\\n{memory_context}\\n\\n\"\n",
    "    \n",
    "    # Add Short-term history\n",
    "    for user_msg, bot_msg in history[-5:]: # Last 5 turns for immediate context\n",
    "        full_prompt += f\"User: {user_msg}\\nBot: {bot_msg}\\n\"\n",
    "    \n",
    "    full_prompt += f\"User: {user_input}\\nBot:\"\n",
    "\n",
    "    # 4. Generate\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=256, \n",
    "            temperature=0.85, \n",
    "            top_p=0.9, \n",
    "            repetition_penalty=1.15,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # 5. Update Memory\n",
    "    memory_system.add_memory(user_id, user_input)\n",
    "    \n",
    "    history.append((user_input, response))\n",
    "    return history, \"\"\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# ðŸ”ž Production NSFW Chatbot\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        scenario = gr.Textbox(label=\"Roleplay Scenario\", placeholder=\"e.g., Teacher/Student, Boss/Secretary\", value=\"Casual chat\")\n",
    "        uid = gr.Textbox(label=\"User ID\", value=\"user_123\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(height=600)\n",
    "    msg = gr.Textbox(label=\"Your Message\")\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    msg.submit(generate_response, [msg, chatbot, scenario, uid], [chatbot, msg])\n",
    "    clear.click(lambda: [], None, chatbot)\n",
    "\n",
    "# Launch\n",
    "if __name__ == '__main__':\n",
    "    demo.queue().launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
